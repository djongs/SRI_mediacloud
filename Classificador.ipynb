{
 "metadata": {
  "name": "",
  "signature": "sha256:bd49591fb8ad3a80e18c7d12f807a883f9a3b98e0d2c9846b77ce492e19c8924"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import numpy as np\n",
      "import os\n",
      "import pymongo\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "client = pymongo.MongoClient()\n",
      "mongo = client.UOL_CRAWLER.urls\n",
      "db = mongo.find({'parsed':{'$exists':True}}, {'cleaned_text':1, 'name':1, '_id':0})\n",
      "labels = []\n",
      "data = []\n",
      "\n",
      "for d in db:\n",
      "    labels.append(d['name'])\n",
      "    data.append(d['cleaned_text'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainset_size = int(round(len(data)*0.75)) # i chose this threshold arbitrarily...to discuss\n",
      "print 'The training set size for this classifier is ' + str(trainset_size) + '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size for this classifier is 8360\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = np.array([''.join(el) for el in data[0:trainset_size]])\n",
      "y_train = np.array([el for el in labels[0:trainset_size]])\n",
      "\n",
      "X_test = np.array([''.join(el) for el in data[trainset_size+1:len(data)]]) \n",
      "y_test = np.array([el for el in labels[trainset_size+1:len(labels)]]) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(min_df=2, \n",
      " ngram_range=(1, 2), \n",
      " stop_words=nltk.corpus.stopwords.words('portuguese'), \n",
      " strip_accents='unicode', \n",
      " norm='l2')\n",
      "\n",
      "test_string = unicode(data[0])\n",
      "\n",
      "#print \"Example string: \" + test_string\n",
      "#print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "#print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "#print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "#print \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)\n",
      "\n",
      "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
      "\n",
      "y_nb_predicted = nb_classifier.predict(X_test)\n",
      "\n",
      "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_nb_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_nb_predicted, labels=np.unique(labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "\n",
        "The precision for this classifier is 0.664370055104\n",
        "The recall for this classifier is 0.170556552962\n",
        "The f1 for this classifier is 0.233747951937\n",
        "The accuracy for this classifier is 0.170556552962\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "     carros       0.88      0.46      0.61       321\n",
        "    ciencia       0.28      0.16      0.20       191\n",
        "     cinema       0.02      1.00      0.04        45\n",
        "  cotidiano       0.33      0.10      0.16       294\n",
        "   economia       0.00      0.00      0.00       263\n",
        "   eleicoes       0.35      0.03      0.06       194\n",
        "    esporte       1.00      0.08      0.16       224\n",
        "internacional       0.85      0.05      0.10       212\n",
        "     musica       1.00      0.28      0.44       326\n",
        "   politica       0.82      0.17      0.28       375\n",
        "      saude       0.86      0.10      0.18       121\n",
        " tecnologia       0.95      0.09      0.16       219\n",
        "\n",
        "avg / total       0.66      0.17      0.23      2785\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[149   3 169   0   0   0   0   0   0   0   0   0]\n",
        " [  0  30 160   1   0   0   0   0   0   0   0   0]\n",
        " [  0   0  45   0   0   0   0   0   0   0   0   0]\n",
        " [  1   6 253  30   0   0   0   1   0   1   2   0]\n",
        " [  3   8 211  38   0   0   0   1   0   1   0   1]\n",
        " [  0   1 174   1   0   6   0   0   0  12   0   0]\n",
        " [  2   4 199   0   0   0  19   0   0   0   0   0]\n",
        " [  0   4 195   1   0   1   0  11   0   0   0   0]\n",
        " [  0   1 234   0   0   0   0   0  91   0   0   0]\n",
        " [  1  10 279  12   0  10   0   0   0  63   0   0]\n",
        " [  0   9  97   3   0   0   0   0   0   0  12   0]\n",
        " [ 14  31 151   4   0   0   0   0   0   0   0  19]]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/evandro/.virtualenvs/goose/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:123: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
        "  tokens = [w for w in tokens if w not in stop_words]\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "svm_classifier = LinearSVC().fit(X_train, y_train)\n",
      "\n",
      "y_svm_predicted = svm_classifier.predict(X_test)\n",
      "print \"MODEL: Linear SVC\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_svm_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_svm_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_svm_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_svm_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_svm_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_svm_predicted, labels=np.unique(labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Linear SVC\n",
        "\n",
        "The precision for this classifier is 0.709435541825\n",
        "The recall for this classifier is 0.250628366248\n",
        "The f1 for this classifier is 0.289285468631\n",
        "The accuracy for this classifier is 0.250628366248\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "     carros       0.98      0.47      0.63       321\n",
        "    ciencia       0.64      0.14      0.23       191\n",
        "     cinema       0.40      0.18      0.25        45\n",
        "  cotidiano       0.48      0.11      0.18       294\n",
        "   economia       0.75      0.10      0.18       263\n",
        "   eleicoes       0.38      0.06      0.11       194\n",
        "    esporte       1.00      0.14      0.25       224\n",
        "internacional       0.76      0.08      0.14       212\n",
        "     musica       0.96      0.33      0.49       326\n",
        "   politica       0.81      0.17      0.29       375\n",
        "      saude       0.73      0.20      0.31       121\n",
        " tecnologia       0.09      0.90      0.17       219\n",
        "\n",
        "avg / total       0.71      0.25      0.29      2785\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[150   1   0   1   0   0   0   0   0   0   0 169]\n",
        " [  0  27   0   4   0   0   0   0   1   0   2 157]\n",
        " [  0   0   8   0   0   0   0   0   0   0   0  37]\n",
        " [  0   2   0  33   0   1   0   1   0   2   5 250]\n",
        " [  1   2   1  19  27   1   0   2   0   0   0 210]\n",
        " [  0   0   0   0   0  12   0   0   0  12   0 170]\n",
        " [  1   0   0   0   0   0  32   1   0   0   0 190]\n",
        " [  0   1   1   1   0   1   0  16   1   1   0 190]\n",
        " [  0   1   5   0   0   0   0   0 107   0   0 213]\n",
        " [  0   2   1   8   4  17   0   0   0  65   1 277]\n",
        " [  0   1   0   0   0   0   0   0   1   0  24  95]\n",
        " [  1   5   4   3   5   0   0   1   2   0   1 197]]\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "maxent_classifier = LogisticRegression().fit(X_train, y_train)\n",
      "\n",
      "y_maxent_predicted = maxent_classifier.predict(X_test)\n",
      "print \"MODEL: Maximum Entropy\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_maxent_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_maxent_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_maxent_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_maxent_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_maxent_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_maxent_predicted, labels=np.unique(labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Maximum Entropy\n",
        "\n",
        "The precision for this classifier is 0.690534730981\n",
        "The recall for this classifier is 0.241292639138\n",
        "The f1 for this classifier is 0.276991376364\n",
        "The accuracy for this classifier is 0.241292639138\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "     carros       0.95      0.46      0.62       321\n",
        "    ciencia       0.55      0.14      0.23       191\n",
        "     cinema       0.38      0.18      0.24        45\n",
        "  cotidiano       0.43      0.12      0.19       294\n",
        "   economia       0.75      0.07      0.13       263\n",
        "   eleicoes       0.38      0.06      0.10       194\n",
        "    esporte       1.00      0.14      0.24       224\n",
        "internacional       0.74      0.07      0.12       212\n",
        "     musica       0.96      0.33      0.49       326\n",
        "   politica       0.82      0.17      0.28       375\n",
        "      saude       0.66      0.17      0.27       121\n",
        " tecnologia       0.09      0.87      0.16       219\n",
        "\n",
        "avg / total       0.69      0.24      0.28      2785\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[148   2   0   2   0   0   0   0   0   0   0 169]\n",
        " [  0  27   0   3   0   0   0   0   1   0   3 157]\n",
        " [  0   0   8   0   0   0   0   0   0   0   0  37]\n",
        " [  0   2   0  35   0   0   0   1   0   1   4 251]\n",
        " [  1   2   2  24  18   1   0   3   0   0   1 211]\n",
        " [  0   0   0   0   0  11   0   0   0  13   0 170]\n",
        " [  1   1   0   0   0   0  31   0   0   0   0 191]\n",
        " [  0   2   1   3   0   1   0  14   0   0   0 191]\n",
        " [  0   1   5   0   0   0   0   0 106   0   0 214]\n",
        " [  0   1   1  10   4  16   0   0   0  63   1 279]\n",
        " [  0   3   0   1   0   0   0   0   1   0  21  95]\n",
        " [  6   8   4   4   2   0   0   1   2   0   2 190]]\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "from itertools import chain\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from operator import itemgetter\n",
      "import re\n",
      "\n",
      "url_pattern = r'https?:\\/\\/(.*[\\r\\n]*)+'\n",
      "\n",
      "documents = data\n",
      "stoplist =  nltk.corpus.stopwords.words('portuguese')\n",
      "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
      "    for document in documents]\n",
      "\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "tfidf = models.TfidfModel(corpus) \n",
      "corpus_tfidf = tfidf[corpus]\n",
      "\n",
      "#lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
      "#lsi.print_topics(20)\n",
      "\n",
      "n_topics = 20\n",
      "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=n_topics)\n",
      "\n",
      "for i in range(0, n_topics):\n",
      "    temp = lda.show_topic(i, 10)\n",
      "    terms = []\n",
      "    for term in temp:\n",
      "        terms.append(term[1])\n",
      "    print \"Top 10 terms for topic #\" + str(i) + \": \"+ \", \".join(terms)\n",
      " \n",
      "print \n",
      "print 'Which LDA topic maximally describes a document?\\n'\n",
      "print 'Original document: ' + documents[1]\n",
      "print 'Preprocessed document: ' + str(texts[1])\n",
      "print 'Matrix Market format: ' + str(corpus[1])\n",
      "print 'Topic probability mixture: ' + str(lda[corpus[1]])\n",
      "print 'Maximally probable topic: topic #' + str(max(lda[corpus[1]],key=itemgetter(1))[0])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 terms for topic #0: dilma, marina, pt, pesquisa, candidata, pontos, inten\u00e7\u00f5es, rousseff, queda, r$\n",
        "Top 10 terms for topic #1: \u00e1lbum, \u2013, r$, dela\u00e7\u00e3o, chery, tartarugas, flex, premiada, sandero, -\n",
        "Top 10 terms for topic #2: xbox, cento, isl\u00e2mico, elei\u00e7\u00f5es, cento,, iraque, gomes, esc\u00e2ndalo, levantamento,, pernambuco,\n",
        "Top 10 terms for topic #3: com\u00edcio, bahia, atacante, senna, ciro, her\u00f3i, servidor, ayrton, guido, queiroz\n",
        "Top 10 terms for topic #4: icloud, sono, umidade, 0,, down, independ\u00eancia,, empr\u00e9stimo, ressecamento, moda, farias"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Top 10 terms for topic #5: apple, questionada, cachorro, tim, disposta, m\u00f3veis, jessica, regulador, p\u00e1gina, concedeu\n",
        "Top 10 terms for topic #6: guardi\u00f5es, telecom, guin\u00e9,, alberto, superman, eva, gal\u00e1xia, pib, pol\u00edtico,, pacote\n",
        "Top 10 terms for topic #7: \u00e9, r$, milh\u00f5es, us$, mil, sa\u00fade, -, segundo, n\u00e3o, j\u00e1\n",
        "Top 10 terms for topic #8: marina,, ex-governador, eduardo, campos,, eleitorado, pal\u00e1cio, \u00e9, sofia, n\u00e3o, (psdb),"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Top 10 terms for topic #9: bernardo, brancos, edison, blues, sony, protestos, in\u00e1cio, quadra, s\u00edndrome, rap\n",
        "Top 10 terms for topic #10: datafolha,, padilha, peemedebista, tratada, pt,, supremo, bilhete, carne, bc,, m\u00eddias\n",
        "Top 10 terms for topic #11: \u00e9, n\u00e3o, s\u00e3o, tamb\u00e9m, ser, est\u00e1, \u00e0, j\u00e1, modelo, nova\n",
        "Top 10 terms for topic #12: dama, sucess\u00e3o, caetano, urso, josh, rosario, alba,, c\u00e1lculo, privado., bluetooth."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Top 10 terms for topic #13: senadores, diego, senado, defender, atl\u00e9tico, embri\u00f5es, gravidez, espanhol, geografia, infec\u00e7\u00f5es\n",
        "Top 10 terms for topic #14: n\u00e3o, \u00e9, governo, ter, hospital, \u00e0, candidato, voc\u00ea, ser, est\u00e1\n",
        "Top 10 terms for topic #15: percentuais, batman, oi, ind\u00edgenas, v, neg\u00f3cio, ibovespa, dawn, soul, voto,\n",
        "Top 10 terms for topic #16: petista, post, ..., tucano, blog, senador, leia, completo, vacina, a\u00e9cio"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Top 10 terms for topic #17: eleitoral, candidato, candidatura, \u00e9, tse, propaganda, (tribunal, blog, arruda, lei\n",
        "Top 10 terms for topic #18: livrarias., previews, bancas, hqs, quadrinhos, lan\u00e7amentos, hist\u00f3rias, cr\u00edticas, novidades, mundo,\n",
        "Top 10 terms for topic #19: a\u00e9cio, marina, psdb, dilma., tucano, ex-senadora, levantamento, dilma, presid\u00eancia,, marina.\n",
        "\n",
        "Which LDA topic maximally describes a document?\n",
        "\n",
        "Original document: WASHINGTON, 16 Set 2014 (AFP) - A ag\u00eancia espacial americana informou nesta ter\u00e7a-feira que far\u00e1 um grande an\u00fancio \u00e0s 16h00 locais (17h00 de Bras\u00edlia) relacionado \u00e0 retomada de voos espaciais tripulados nos Estados Unidos.\n",
        "\n",
        "\n",
        "\n",
        "A Nasa, que tem sido incapaz de enviar pessoas ao espa\u00e7o desde a aposentadoria de seus \u00f4nibus espaciais, em 2011, disse que o an\u00fancio ser\u00e1 feito em uma coletiva de imprensa no Centro Espacial Kennedy, na Fl\u00f3rida, e ser\u00e1 transmitida ao vivo pela emissora de televis\u00e3o da Nasa e por seu site.\n",
        "\n",
        "\n",
        "\n",
        "\"Estamos retornando os voos espaciais tripulados para os Estados Unidos. Saiba quem vai levar as tripula\u00e7\u00f5es \u00e0 ISS (Esta\u00e7\u00e3o Espacial Internacional)\", disse a Nasa pelo Twitter. \n",
        "\n",
        "\n",
        "\n",
        "Um porta-voz da ag\u00eancia n\u00e3o quis fornecer mais detalhes at\u00e9 o an\u00fancio, que est\u00e1 programado para coincidir com o fechamento dos mercados norte-americanos.\n",
        "\n",
        "\n",
        "\n",
        "A ag\u00eancia gastou centenas de milhares de d\u00f3lares para ajudar empresas privadas, como SpaceX, Boeing e Sierra Nevada, a desenvolver seus pr\u00f3prios ve\u00edculos de transporte de tripula\u00e7\u00e3o para que os americanos possam lan\u00e7ar voos \u00e0 ISS em 2017.\n",
        "\n",
        "\n",
        "\n",
        "Nesse per\u00edodo, os astronautas do mundo precisaram contar com as naves Soyuz russas para o transporte ao posto avan\u00e7ado em \u00f3rbita, a um custo de US$ 70 milh\u00f5es por assento. \n",
        "\n",
        "\n",
        "\n",
        "The Wall Street Journal citou fontes do setor n\u00e3o identificadas que teriam afirmado que a Boeing era considerada uma das favoritas nas propostas da Nasa.\n",
        "Preprocessed document: [u'washington,', u'16', u'set', u'2014', u'(afp)', u'-', u'ag\\xeancia', u'espacial', u'americana', u'informou', u'nesta', u'ter\\xe7a-feira', u'far\\xe1', u'grande', u'an\\xfancio', u'\\xe0s', u'16h00', u'locais', u'(17h00', u'bras\\xedlia)', u'relacionado', u'\\xe0', u'retomada', u'voos', u'espaciais', u'tripulados', u'estados', u'unidos.', u'nasa,', u'sido', u'incapaz', u'enviar', u'pessoas', u'espa\\xe7o', u'desde', u'aposentadoria', u'\\xf4nibus', u'espaciais,', u'2011,', u'disse', u'an\\xfancio', u'ser\\xe1', u'feito', u'coletiva', u'imprensa', u'centro', u'espacial', u'kennedy,', u'fl\\xf3rida,', u'ser\\xe1', u'transmitida', u'vivo', u'emissora', u'televis\\xe3o', u'nasa', u'site.', u'\"estamos', u'retornando', u'voos', u'espaciais', u'tripulados', u'estados', u'unidos.', u'saiba', u'vai', u'levar', u'tripula\\xe7\\xf5es', u'\\xe0', u'iss', u'(esta\\xe7\\xe3o', u'espacial', u'internacional)\",', u'disse', u'nasa', u'twitter.', u'porta-voz', u'ag\\xeancia', u'n\\xe3o', u'quis', u'fornecer', u'detalhes', u'at\\xe9', u'an\\xfancio,', u'est\\xe1', u'programado', u'coincidir', u'fechamento', u'mercados', u'norte-americanos.', u'ag\\xeancia', u'gastou', u'centenas', u'milhares', u'd\\xf3lares', u'ajudar', u'empresas', u'privadas,', u'spacex,', u'boeing', u'sierra', u'nevada,', u'desenvolver', u'pr\\xf3prios', u've\\xedculos', u'transporte', u'tripula\\xe7\\xe3o', u'americanos', u'possam', u'lan\\xe7ar', u'voos', u'\\xe0', u'iss', u'2017.', u'nesse', u'per\\xedodo,', u'astronautas', u'mundo', u'precisaram', u'contar', u'naves', u'soyuz', u'russas', u'transporte', u'posto', u'avan\\xe7ado', u'\\xf3rbita,', u'custo', u'us$', u'70', u'milh\\xf5es', u'assento.', u'the', u'wall', u'street', u'journal', u'citou', u'fontes', u'setor', u'n\\xe3o', u'identificadas', u'afirmado', u'boeing', u'considerada', u'favoritas', u'propostas', u'nasa.']\n",
        "Matrix Market format: [(41, 1), (60, 2), (101, 2), (163, 3), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 3), (178, 1), (179, 1), (180, 1), (181, 2), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 2), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 2), (205, 1), (206, 3), (207, 1), (208, 2), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 2), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 2), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 2), (272, 2), (273, 1), (274, 1), (275, 1), (276, 2), (277, 1), (278, 1), (279, 1), (280, 1), (281, 3), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1)]\n",
        "Topic probability mixture: [(0, 0.050878553967219547), (4, 0.088479276488718936), (7, 0.3315896474672535), (10, 0.013960796391957478), (11, 0.14664971463481019), (14, 0.24785572206899292), (15, 0.019423296140916001), (17, 0.061062641782005468), (19, 0.014095469351345298)]\n",
        "Maximally probable topic: topic #7\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}